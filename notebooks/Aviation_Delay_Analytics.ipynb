{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Data Acquisition & Preprocessing"
      ],
      "metadata": {
        "id": "DQNjBXvOiY9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3W5g6ijfDTv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the main dataset\n",
        "# Ensure this path matches where you saved 'flights.csv' in your Drive\n",
        "path = '/content/drive/MyDrive/dataset/flights.csv'\n",
        "df = pd.read_csv(path, low_memory=False)\n",
        "\n",
        "print(f\"Dataset loaded with {df.shape[0]} rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target: 1 if delayed, 0 if on time/early\n",
        "# We use ARRIVAL_DELAY to create our classification target\n",
        "df['is_delayed'] = (df['ARRIVAL_DELAY'] > 0).astype(int)\n",
        "\n",
        "# Data Cleaning: Drop rows where ARRIVAL_DELAY is NaN so we have a clean target\n",
        "df = df.dropna(subset=['ARRIVAL_DELAY'])\n",
        "\n",
        "print(\"Step 2 Complete: Target variable 'is_delayed' created.\")"
      ],
      "metadata": {
        "id": "5UD1mkFWfm2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify valid numerical features (Excluding 'is_delayed' and direct delay metrics)\n",
        "# These are columns available BEFORE the flight arrives\n",
        "cols_to_exclude = ['ARRIVAL_DELAY', 'is_delayed', 'CANCELLED', 'DIVERTED',\n",
        "                   'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY',\n",
        "                   'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY', 'DEPARTURE_DELAY']\n",
        "\n",
        "numeric_df = df.drop(columns=cols_to_exclude).select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate Correlation\n",
        "correlations = numeric_df.corrwith(df['is_delayed']).abs().sort_values(ascending=False)\n",
        "\n",
        "# Select Top 10\n",
        "top_10_features = correlations.head(10).index.tolist()\n",
        "print(\"Your Top 10 Features are:\")\n",
        "for i, feat in enumerate(top_10_features, 1):\n",
        "    print(f\"{i}. {feat}\")\n",
        "\n",
        "# Keep only these 10 features + our target\n",
        "df_final = df[top_10_features + ['is_delayed']]"
      ],
      "metadata": {
        "id": "ke0sVrKQf3YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(df_final.drop('is_delayed', axis=1))\n",
        "y = df_final['is_delayed'].values\n",
        "\n",
        "# Handling Imbalance\n",
        "# Combine for resampling\n",
        "temp_df = pd.concat([pd.DataFrame(X, columns=top_10_features),\n",
        "                     df_final['is_delayed'].reset_index(drop=True)], axis=1)\n",
        "\n",
        "df_majority = temp_df[temp_df.is_delayed == 0]\n",
        "df_minority = temp_df[temp_df.is_delayed == 1]\n",
        "\n",
        "# Downsample majority to match minority\n",
        "df_balanced = resample(df_majority, replace=False,\n",
        "                       n_samples=len(df_minority),\n",
        "                       random_state=42)\n",
        "\n",
        "df_final_balanced = pd.concat([df_balanced, df_minority])\n",
        "\n",
        "print(\"Data is scaled and balanced.\")\n",
        "print(df_final_balanced['is_delayed'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "ZSVmSH9Of8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "L0Ru5ks6ibp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix of our balanced dataset\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = df_final_balanced.corr()\n",
        "\n",
        "# Plot Heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='RdBu', fmt='.2f', center=0)\n",
        "plt.title('Correlation Heatmap of Selected Features')\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation heatmap generated.\")"
      ],
      "metadata": {
        "id": "DROLcYSHihKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Analysis: Distribution of the target variable\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x='is_delayed', data=df_final_balanced, palette='viridis')\n",
        "plt.title('Class Distribution (Balanced)')\n",
        "\n",
        "# Bivariate Analysis: Feature vs Target\n",
        "# See how TAXI_OUT affects delays\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='is_delayed', y='TAXI_OUT', data=df_final_balanced)\n",
        "plt.title('Taxi-Out Time vs Flight Delay')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical Summaries as required by the instructions\n",
        "print(\"\\n--- Statistical Summary of Features ---\")\n",
        "print(df_final_balanced.describe())"
      ],
      "metadata": {
        "id": "fdLSeZpqjeaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Prepare Features (X) and Target (y) from the balanced dataframe\n",
        "X = df_final_balanced.drop('is_delayed', axis=1)\n",
        "y = df_final_balanced['is_delayed']\n",
        "\n",
        "# 2. Split into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Create a smaller sample for computationally heavy models (KNN, SVM, RF)\n",
        "# We take 20,000 rows from the training set to ensure it runs quickly in Colab\n",
        "X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=20000, random_state=42)\n",
        "\n",
        "print(\"Step Complete: Data split into Training and Testing sets.\")\n",
        "print(f\"Full Train set: {X_train.shape[0]} rows\")\n",
        "print(f\"Small Train sample: {X_train_small.shape[0]} rows\")"
      ],
      "metadata": {
        "id": "A-ZWGlfM-PsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Machine Learning Analysis & Model Building"
      ],
      "metadata": {
        "id": "6R_39HF4kjac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "\n",
        "# Initialize the results list - only do this ONCE\n",
        "results = []\n",
        "print(\"Metrics imported and results list initialized.\")"
      ],
      "metadata": {
        "id": "GY_N9wVK25Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the models\n",
        "base_models = {\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "for name, model in base_models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds),\n",
        "        \"Recall\": recall_score(y_test, preds),\n",
        "        \"F1-Score\": f1_score(y_test, preds)\n",
        "    })\n",
        "\n",
        "print(\"Base models trained and added to results.\")"
      ],
      "metadata": {
        "id": "tUKWTdV828_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Define the models\n",
        "slow_models = {\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"SVM\": LinearSVC(max_iter=1000, dual=False)\n",
        "}\n",
        "\n",
        "# Train and evaluate using the small sample\n",
        "for name, model in slow_models.items():\n",
        "    print(f\"Training {name} on sample...\")\n",
        "    model.fit(X_train_small, y_train_small)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds),\n",
        "        \"Recall\": recall_score(y_test, preds),\n",
        "        \"F1-Score\": f1_score(y_test, preds)\n",
        "    })\n",
        "\n",
        "print(\"KNN and SVM trained on sample.\")"
      ],
      "metadata": {
        "id": "GS33F4mY3Oo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define and train\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model.fit(X_train_small, y_train_small)\n",
        "\n",
        "# Evaluate\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "results.append({\n",
        "    \"Model\": \"Random Forest\",\n",
        "    \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
        "    \"Precision\": precision_score(y_test, rf_preds),\n",
        "    \"Recall\": recall_score(y_test, rf_preds),\n",
        "    \"F1-Score\": f1_score(y_test, rf_preds)\n",
        "})\n",
        "\n",
        "print(\"Random Forest trained.\")"
      ],
      "metadata": {
        "id": "d0FZd50b3bZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison_df = pd.DataFrame(results)\n",
        "print(\"\\n--- Final Model Comparison Table ---\")\n",
        "display(comparison_df)"
      ],
      "metadata": {
        "id": "2PA6jN8g3k9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Combine all trained models for plotting\n",
        "plot_models = {\n",
        "    \"Naive Bayes\": base_models[\"Naive Bayes\"],\n",
        "    \"Logistic Regression\": base_models[\"Logistic Regression\"],\n",
        "    \"KNN\": slow_models[\"KNN\"],\n",
        "    \"Random Forest\": rf_model\n",
        "}\n",
        "\n",
        "for name, model in plot_models.items():\n",
        "    # LinearSVC (SVM) uses decision_function instead of predict_proba\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        probs = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        probs = model.decision_function(X_test)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc(fpr, tpr):.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Model Performance: ROC Curves')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNTmygiU3ng7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to your Drive path (Double check the folder exists!)\n",
        "drive_path = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "joblib.dump(rf_model, drive_path + 'best_model.pkl')\n",
        "joblib.dump(scaler, drive_path + 'scaler.pkl')\n",
        "\n",
        "print(f\"Exported: 'best_model.pkl' and 'scaler.pkl' saved to {drive_path}\")"
      ],
      "metadata": {
        "id": "HeAbCrBA33qR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}